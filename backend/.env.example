# Backend Environment Variables

# GitHub Personal Access Token (optional, for higher API rate limits)
# Get yours at: https://github.com/settings/tokens
GITHUB_TOKEN=

# Model Configuration
# Path to the CodeLlama GGUF model file
MODEL_PATH=./models/codellama-7b-instruct.Q4_K_M.gguf

# Indexing Configuration
MAX_FILE_SIZE=1048576  # 1MB in bytes
CHUNK_SIZE=1000  # Characters per chunk
CHUNK_OVERLAP=200  # Characters overlap between chunks

# RAG Configuration
TOP_K_RESULTS=5  # Number of relevant code snippets to retrieve
MAX_CONTEXT_LENGTH=4000  # Maximum context length for LLM

# Server Configuration
HOST=0.0.0.0
PORT=8000
